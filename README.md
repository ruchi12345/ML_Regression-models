# ML_Regression-models
## Machine learning:

Machine learning is a part of artificial intelligence (AI) that provides systems the ability to automatically learn and improve from experience.The main focus of machine learning is to develop the computer programs.ML are those that can learn from data and improve from experience, without human intervention.Â Learning tasks may include learning the function that maps the input to the output. To select the right alogritm is key part of ml.

## Supervised learning:
supervised learning use labeled training data to learn the mapping function from the input variable to the output variable.

Supervised learning can be of two types:

## 1.Classification:
To predict the outcome of a given sample where the output variable is in the form of categories
## 2.Regression:
To predict the outcome of a given sample where the output variable is in the form of real values.

## The algorithms we cover in this blog is:
1.Linear Regression

2.Support vector machine with regression(SVR)

3.Random Forest Regression

4.Decision Tree Regressor  are the example of supervised learning.

## Linear Regression:

This Regression algorithm finds the relationship between dependent variable and independent variable. We have set of input variable that are used to identify the output variable.The goal of ML is to quantify this relationship.
Linear Regression is represented as a line in the form of y = a + bx.

## Support vector Machine-Regression(SVR):

Support Vector Machine can also be used as a regression method,The Support Vector Regression (SVR) uses the same principles as the SVM for classification, with only a few minor differences.First of all, because output is a real number it becomes very difficult to predict the information at hand, which has infinite possibilities.It create hyperpalne with maximum margin from support vectors.

## Random Forest Regression:

Random forest creates multiple decision trees on randomly selected data samples, then merge that trees for the accurate and stable output. It is flexible and easy to use alogrihtm.As the name suggest it create forest of trees,features are the root nodes and subnodes are the trees created by the random forest.

## Decision Tree Regressor:

A decision tree is graphical represention of all the possible solutions to a decision tree based on certain conditions. It is decision tress because it start with a single root it create tree for the best fir output.

## Polynomial Regression:

It is a technique to fit a nonlinear equation by taking polynomial functions of independent variable.There are some relationships that a researcher will hypothesize is curvilinear. Clearly, such type of cases will include a polynomial term.

## Ridge Regression:

Ridge regression is for reduce the complexity of model that is number of predictors.Removing predictors from the model can be seen as settings their coefficients to zero. Instead of forcing them to be exactly zero.

## Lasso Regression:

Lasso regression is as same as the ridge regression  both used for the reduce the complexity of model.The only difference 
instead of taking the square of the coefficients, magnitudes are taken into account. some of the features are completely neglected for the evaluation of output. So Lasso regression not only helps in reducing over-fitting but it can help us in feature selection.

## Elastic net Regression:

The method linearly combines the L1 and L2 penalties of the LASSO and Ridge Regression. Including the Elastic Net, these methods are especially powerful when applied to very large data where the number of variables might be in the thousands or even millions.

## Bayesian regression:

Bayesian regression techniques can be used to include regularization parameters in the estimation procedure,the regularization parameter is not set in a hard sense but tuned to the data at hand.

## Adaboost regression:

It is Adaptive boasting Algorithm aims to convert a set of weak learners into a strong one. It starts by predicting original data set and gives equal weight to each observation. If prediction is incorrect using the first learner, then it gives higher weight to observation which have been predicted incorrectly.

